{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the next strategies in steps: ###\n",
    "\n",
    "- **Fill in missing weather data**\n",
    "    Original code from Kaggle notebook :\n",
    "    https://www.kaggle.com/aitude/ashrae-missing-weather-data-handling by @aitude\n",
    "\n",
    "\n",
    "- **Data cleanup**\n",
    "     Use data cleanup strategy from Kaggle notebook : \n",
    "     https://www.kaggle.com/purist1024/ashrae-simple-data-cleanup-lb-1-08-no-leaks by @purist1024\n",
    "     \n",
    "     - Drop rows with unjustified zeros\n",
    "     - Site0 rows - the first 141 rows\n",
    "     - Abnormally high reading from building 1099\n",
    "     - Zero readings from electrical meters   \n",
    "\n",
    "\n",
    "- **Feature engineering**\n",
    "    - Time features: weekday, weekend, month, is_holiday\n",
    "    - Feels Like temperature\n",
    "    \n",
    "\n",
    "- **LightGBM**\n",
    "    - Use Bayesian optimization for parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import datetime\n",
    "from meteocalc import feels_like\n",
    "import gc\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_PATH = \"../../Ashrae/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "building_df = pd.read_csv(DATA_PATH + 'building_metadata.csv')\n",
    "weather_df = pd.read_csv(DATA_PATH + 'weather_train.csv')\n",
    "\n",
    "test_df = pd.read_csv(DATA_PATH + 'test.csv')\n",
    "weather_test_df = pd.read_csv(DATA_PATH + 'weather_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Fill in missing weather data**\n",
    "Original code from:  https://www.kaggle.com/aitude/ashrae-missing-weather-data-handling by @aitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original code from https://www.kaggle.com/aitude/ashrae-missing-weather-data-handling by @aitude\n",
    "\n",
    "def fill_weather_dataset(weather_df):\n",
    "    \n",
    "    # Find Missing Dates\n",
    "    time_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_date = datetime.datetime.strptime(weather_df['timestamp'].min(),time_format)\n",
    "    end_date = datetime.datetime.strptime(weather_df['timestamp'].max(),time_format)\n",
    "    total_hours = int(((end_date - start_date).total_seconds() + 3600) / 3600)\n",
    "    hours_list = [(end_date - datetime.timedelta(hours=x)).strftime(time_format) for x in range(total_hours)]\n",
    "\n",
    "    missing_hours = []\n",
    "    for site_id in range(16):\n",
    "        site_hours = np.array(weather_df[weather_df['site_id'] == site_id]['timestamp'])\n",
    "        new_rows = pd.DataFrame(np.setdiff1d(hours_list,site_hours),columns=['timestamp'])\n",
    "        new_rows['site_id'] = site_id\n",
    "        weather_df = pd.concat([weather_df,new_rows])\n",
    "\n",
    "        weather_df = weather_df.reset_index(drop=True)           \n",
    "\n",
    "    # Add new Features\n",
    "    weather_df[\"datetime\"] = pd.to_datetime(weather_df[\"timestamp\"])\n",
    "    weather_df[\"day\"] = weather_df[\"datetime\"].dt.weekday\n",
    "    weather_df[\"week\"] = weather_df[\"datetime\"].dt.week\n",
    "    weather_df[\"month\"] = weather_df[\"datetime\"].dt.month\n",
    "    weather_df[\"hour\"] = weather_df[\"datetime\"].dt.hour\n",
    "    \n",
    "    # Reset Index for Fast Update\n",
    "    weather_df = weather_df.set_index(['site_id','day','month'])\n",
    "\n",
    "    air_temperature_filler = pd.DataFrame(weather_df.groupby(['site_id','day','month'])['air_temperature'].mean(),columns=[\"air_temperature\"])\n",
    "    weather_df.update(air_temperature_filler,overwrite=False)\n",
    "\n",
    "    # Step 1\n",
    "    cloud_coverage_filler = weather_df.groupby(['site_id','day','month'])['cloud_coverage'].mean()\n",
    "    # Step 2\n",
    "    cloud_coverage_filler = pd.DataFrame(cloud_coverage_filler.fillna(method='ffill'),columns=[\"cloud_coverage\"])\n",
    "\n",
    "    weather_df.update(cloud_coverage_filler,overwrite=False)\n",
    "\n",
    "    due_temperature_filler = pd.DataFrame(weather_df.groupby(['site_id','day','month'])['dew_temperature'].mean(),columns=[\"dew_temperature\"])\n",
    "    weather_df.update(due_temperature_filler,overwrite=False)\n",
    "\n",
    "    # Step 1\n",
    "    sea_level_filler = weather_df.groupby(['site_id','day','month'])['sea_level_pressure'].mean()\n",
    "    # Step 2\n",
    "    sea_level_filler = pd.DataFrame(sea_level_filler.fillna(method='ffill'),columns=['sea_level_pressure'])\n",
    "\n",
    "    weather_df.update(sea_level_filler,overwrite=False)\n",
    "\n",
    "    wind_direction_filler =  pd.DataFrame(weather_df.groupby(['site_id','day','month'])['wind_direction'].mean(),columns=['wind_direction'])\n",
    "    weather_df.update(wind_direction_filler,overwrite=False)\n",
    "\n",
    "    wind_speed_filler =  pd.DataFrame(weather_df.groupby(['site_id','day','month'])['wind_speed'].mean(),columns=['wind_speed'])\n",
    "    weather_df.update(wind_speed_filler,overwrite=False)\n",
    "\n",
    "    # Step 1\n",
    "    precip_depth_filler = weather_df.groupby(['site_id','day','month'])['precip_depth_1_hr'].mean()\n",
    "    # Step 2\n",
    "    precip_depth_filler = pd.DataFrame(precip_depth_filler.fillna(method='ffill'),columns=['precip_depth_1_hr'])\n",
    "\n",
    "    weather_df.update(precip_depth_filler,overwrite=False)\n",
    "\n",
    "    weather_df = weather_df.reset_index()\n",
    "    weather_df = weather_df.drop(['datetime', 'week'],axis=1)\n",
    "        \n",
    "    return weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meteorological_features(data):\n",
    "        def calculate_rh(df):\n",
    "            df['relative_humidity'] = 100 * (np.exp((17.625 * df['dew_temperature']) / (243.04 + df['dew_temperature'])) / np.exp((17.625 * df['air_temperature'])/(243.04 + df['air_temperature'])))\n",
    "        def calculate_fl(df):\n",
    "            flike_final = []\n",
    "            flike = []\n",
    "            # calculate Feels Like temperature\n",
    "            for i in range(len(df)):\n",
    "                at = df['air_temperature'][i]\n",
    "                rh = df['relative_humidity'][i]\n",
    "                ws = df['wind_speed'][i]\n",
    "                flike.append(feels_like(at, rh, ws))\n",
    "            for i in range(len(flike)):\n",
    "                flike_final.append(flike[i].f)\n",
    "            df['feels_like'] = flike_final\n",
    "            del flike_final, flike, at, rh, ws\n",
    "        calculate_rh(data)\n",
    "        calculate_fl(data)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = fill_weather_dataset(weather_df)\n",
    "weather_test_df = fill_weather_dataset(weather_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = get_meteorological_features(weather_df)\n",
    "weather_test_df = get_meteorological_features(weather_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Reduce memory usage**\n",
    "Original code from: https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original code from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin\n",
    "\n",
    "def reduce_mem_usage(df, use_float16=False):\n",
    "    \"\"\"\n",
    "    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    \n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
    "            continue\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
    "    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 616.95 MB\n",
      "Memory usage after optimization is: 173.90 MB\n",
      "Decreased by 71.8%\n",
      "Memory usage of dataframe is 0.07 MB\n",
      "Memory usage after optimization is: 0.02 MB\n",
      "Decreased by 73.8%\n",
      "Memory usage of dataframe is 15.01 MB\n",
      "Memory usage after optimization is: 3.60 MB\n",
      "Decreased by 76.0%\n",
      "Memory usage of dataframe is 1272.51 MB\n",
      "Memory usage after optimization is: 358.65 MB\n",
      "Decreased by 71.8%\n",
      "Memory usage of dataframe is 29.94 MB\n",
      "Memory usage after optimization is: 7.17 MB\n",
      "Decreased by 76.0%\n"
     ]
    }
   ],
   "source": [
    "train_df = reduce_mem_usage(train_df,use_float16=True)\n",
    "building_df = reduce_mem_usage(building_df,use_float16=True)\n",
    "weather_df = reduce_mem_usage(weather_df,use_float16=True)\n",
    "\n",
    "test_df = reduce_mem_usage(test_df, use_float16 = True)\n",
    "weather_test_df = reduce_mem_usage(weather_test_df, use_float16 = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Merge data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.merge(building_df, left_on='building_id',right_on='building_id',how='left')\n",
    "train_df = train_df.merge(weather_df,how='left',left_on=['site_id','timestamp'],right_on=['site_id','timestamp'])\n",
    "\n",
    "test_df = test_df.merge(building_df, left_on='building_id',right_on='building_id',how='left')\n",
    "test_df = test_df.merge(weather_test_df,how='left',left_on=['site_id','timestamp'],right_on=['site_id','timestamp'])\n",
    "\n",
    "del weather_df, weather_test_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Data Cleanup**\n",
    "Use rows_to_drop dataset from:  https://www.kaggle.com/purist1024/ashrae-simple-data-cleanup-lb-1-08-no-leaks by @purist1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in rows_to_drop.csv\n",
    "\n",
    "rows_to_drop = pd.read_csv(DATA_PATH + 'rows_to_drop.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(rows_to_drop.loc[:, '0'], inplace = True)\n",
    "train_df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Feature engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_engineering(df):\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    df.sort_values(\"timestamp\")\n",
    "    df.reset_index(drop=True)\n",
    "    \n",
    "    # Add more features\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"],format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    df[\"weekday\"] = df[\"timestamp\"].dt.weekday\n",
    "    df[\"weekend\"] = df['weekday'].apply(lambda day_: 1 if day_>5 else 0)\n",
    "    \n",
    "    holidays = [\"2016-01-01\", \"2016-01-18\", \"2016-02-15\", \"2016-05-30\", \"2016-07-04\",\n",
    "                    \"2016-09-05\", \"2016-10-10\", \"2016-11-11\", \"2016-11-24\", \"2016-12-26\",\n",
    "                    \"2017-01-02\", \"2017-01-16\", \"2017-02-20\", \"2017-05-29\", \"2017-07-04\",\n",
    "                    \"2017-09-04\", \"2017-10-09\", \"2017-11-10\", \"2017-11-23\", \"2017-12-25\",\n",
    "                    \"2018-01-01\", \"2018-01-15\", \"2018-02-19\", \"2018-05-28\", \"2018-07-04\",\n",
    "                    \"2018-09-03\", \"2018-10-08\", \"2018-11-12\", \"2018-11-22\", \"2018-12-25\",\n",
    "                    \"2019-01-01\"]\n",
    "    \n",
    "    df[\"is_holiday\"] = (df.timestamp.isin(holidays)).astype(int)\n",
    "    df['square_feet'] =  np.log1p(df['square_feet'])\n",
    "    \n",
    "    # Remove Unused Columns\n",
    "    drop = [\"timestamp\"]\n",
    "    df = df.drop(drop, axis=1)\n",
    "    gc.collect()\n",
    "    \n",
    "    # Encode Categorical Data\n",
    "    le = LabelEncoder()\n",
    "    df[\"primary_use\"] = le.fit_transform(df[\"primary_use\"])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = features_engineering(train_df)\n",
    "test_df = features_engineering(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>meter</th>\n",
       "      <th>meter_reading</th>\n",
       "      <th>site_id</th>\n",
       "      <th>primary_use</th>\n",
       "      <th>square_feet</th>\n",
       "      <th>year_built</th>\n",
       "      <th>floor_count</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>...</th>\n",
       "      <th>precip_depth_1_hr</th>\n",
       "      <th>sea_level_pressure</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>hour</th>\n",
       "      <th>relative_humidity</th>\n",
       "      <th>feels_like</th>\n",
       "      <th>weekday</th>\n",
       "      <th>weekend</th>\n",
       "      <th>is_holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.913685</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.689453</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>73.8125</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "      <td>23.303600</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10.832181</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041656</td>\n",
       "      <td>1021.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>3.099609</td>\n",
       "      <td>0</td>\n",
       "      <td>90.5625</td>\n",
       "      <td>-2.794922</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>0.374600</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.589514</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041656</td>\n",
       "      <td>1021.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>3.099609</td>\n",
       "      <td>0</td>\n",
       "      <td>90.5625</td>\n",
       "      <td>-2.794922</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.589514</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041656</td>\n",
       "      <td>1021.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>3.099609</td>\n",
       "      <td>0</td>\n",
       "      <td>90.5625</td>\n",
       "      <td>-2.794922</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>175.184006</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.487946</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041656</td>\n",
       "      <td>1021.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>3.099609</td>\n",
       "      <td>0</td>\n",
       "      <td>90.5625</td>\n",
       "      <td>-2.794922</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   building_id  meter  meter_reading  site_id  primary_use  square_feet  \\\n",
       "0            0      0       0.000000        0            0     8.913685   \n",
       "1          105      0      23.303600        1            0    10.832181   \n",
       "2          106      0       0.374600        1            0     8.589514   \n",
       "3          106      3       0.000000        1            0     8.589514   \n",
       "4          107      0     175.184006        1            0    11.487946   \n",
       "\n",
       "   year_built  floor_count  day  month  ...  precip_depth_1_hr  \\\n",
       "0      2008.0          NaN    4      1  ...           2.689453   \n",
       "1         NaN          5.0    4      1  ...           0.041656   \n",
       "2         NaN          4.0    4      1  ...           0.041656   \n",
       "3         NaN          4.0    4      1  ...           0.041656   \n",
       "4      2005.0         10.0    4      1  ...           0.041656   \n",
       "\n",
       "   sea_level_pressure  wind_direction  wind_speed  hour  relative_humidity  \\\n",
       "0              1019.5             0.0    0.000000     0            73.8125   \n",
       "1              1021.0           240.0    3.099609     0            90.5625   \n",
       "2              1021.0           240.0    3.099609     0            90.5625   \n",
       "3              1021.0           240.0    3.099609     0            90.5625   \n",
       "4              1021.0           240.0    3.099609     0            90.5625   \n",
       "\n",
       "   feels_like  weekday  weekend  is_holiday  \n",
       "0   25.000000        4        0           0  \n",
       "1   -2.794922        4        0           0  \n",
       "2   -2.794922        4        0           0  \n",
       "3   -2.794922        4        0           0  \n",
       "4   -2.794922        4        0           0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('train_final.csv')\n",
    "test_df.to_csv('test_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "building_id                  0\n",
       "meter                        0\n",
       "meter_reading                0\n",
       "site_id                      0\n",
       "primary_use                  0\n",
       "square_feet                  0\n",
       "year_built            11685636\n",
       "floor_count           15811110\n",
       "day                          0\n",
       "month                        0\n",
       "air_temperature              0\n",
       "cloud_coverage               0\n",
       "dew_temperature              0\n",
       "precip_depth_1_hr            0\n",
       "sea_level_pressure           0\n",
       "wind_direction               0\n",
       "wind_speed                   0\n",
       "hour                         0\n",
       "relative_humidity            0\n",
       "feels_like                   0\n",
       "weekday                      0\n",
       "weekend                      0\n",
       "is_holiday                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "row_id                       0\n",
       "building_id                  0\n",
       "meter                        0\n",
       "site_id                      0\n",
       "primary_use                  0\n",
       "square_feet                  0\n",
       "year_built            24598080\n",
       "floor_count           34444320\n",
       "day                          0\n",
       "month                        0\n",
       "air_temperature              0\n",
       "cloud_coverage               0\n",
       "dew_temperature              0\n",
       "precip_depth_1_hr            0\n",
       "sea_level_pressure           0\n",
       "wind_direction               0\n",
       "wind_speed                   0\n",
       "hour                         0\n",
       "relative_humidity            0\n",
       "feels_like                   0\n",
       "weekday                      0\n",
       "weekend                      0\n",
       "is_holiday                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The two column with null values do not have a high feature importance, so try dropping them\n",
    "\n",
    "col_to_drop = ['year_built', 'floor_count']\n",
    "\n",
    "train = train_df.drop(columns = col_to_drop)\n",
    "test = test_df.drop(columns = col_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
